{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-15T14:26:45.174272Z",
     "start_time": "2024-05-15T14:26:45.139165Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils import shuffle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, ParameterGrid\n",
    "from sklearn.metrics import accuracy_score, classification_report"
   ],
   "id": "d6728372138d3e34",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-15T14:46:50.660078Z",
     "start_time": "2024-05-15T14:46:47.663779Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data = np.load(\"../../Data/development.npy\")\n",
    "data_csv = pd.read_csv(\"../../Data/metadata/development.csv\")\n",
    "idx_to_feature_name = pd.read_csv(\"../../Data/metadata/idx_to_feature_name.csv\")\n",
    "labels = data_csv['word']\n",
    "y = {}\n",
    "labels_new = []\n",
    "i = 0\n",
    "for label in np.unique(labels).tolist():\n",
    "    if label == 'other':\n",
    "        continue\n",
    "    else:\n",
    "        y[label] = i\n",
    "        i += 1\n",
    "for label in labels:\n",
    "    if label == 'other':\n",
    "        continue\n",
    "    else:\n",
    "        labels_new.append(y[label])\n",
    "data_no_other = data[:40834, :, :]\n",
    "data_train = np.concatenate((data_no_other[:32667, :22, :], data_no_other[:32667, 87:, :]),\n",
    "                                    axis=1)  # data[samples, features, ...]\n",
    "data_valid = np.concatenate((data_no_other[32667:, :22, :], data_no_other[32667:, 87:, :]), axis=1)\n",
    "print(data_train.shape, data_valid.shape)\n",
    "labels_train = labels_new[:32667]\n",
    "labels_valid = labels_new[32667:]"
   ],
   "id": "217ea0aa0e028bbf",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32667, 110, 44) (8167, 110, 44)\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-15T14:29:51.336617Z",
     "start_time": "2024-05-15T14:29:51.239921Z"
    }
   },
   "cell_type": "code",
   "source": [
    "X_train, X_valid, y_train, y_valid = data_train, data_valid, labels_train, labels_valid\n",
    "\n",
    "# Create DataLoader\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Define a simple model\n",
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(SimpleModel, self).__init__()\n",
    "        self.linear = nn.Linear(input_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "# Set device to MPS if available\n",
    "device = torch.device(\"mps\" if torch.has_mps else \"cpu\")\n",
    "\n",
    "# Define the function to train and evaluate the model\n",
    "def train_and_evaluate(params):\n",
    "    model = SimpleModel(input_dim=X_train.shape[1]).to(device)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=params['lr'], weight_decay=params['weight_decay'])\n",
    "\n",
    "    # Training loop\n",
    "    model.train()\n",
    "    for epoch in range(params['epochs']):\n",
    "        for batch_X, batch_y in train_loader:\n",
    "            batch_X, batch_y = batch_X.to(device), batch_y.to(device).float()\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_X).squeeze()\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        X_valid_device = X_valid.to(device)\n",
    "        y_valid_device = y_valid.to(device).float()\n",
    "        outputs = model(X_valid_device).squeeze()\n",
    "        preds = (torch.sigmoid(outputs) > 0.5).int()\n",
    "        accuracy = accuracy_score(y_valid_device.cpu(), preds.cpu())\n",
    "        report = classification_report(y_valid_device.cpu(), preds.cpu())\n",
    "    \n",
    "    return accuracy, report\n",
    "\n",
    "# Define parameter grid\n",
    "param_grid = {\n",
    "    'lr': [0.001, 0.01, 0.1],\n",
    "    'weight_decay': [0.0001, 0.001, 0.01],\n",
    "    'epochs': [10, 20, 30]\n",
    "}\n",
    "\n",
    "# Perform grid search manually\n",
    "best_params = None\n",
    "best_accuracy = 0\n",
    "best_report = None\n",
    "\n",
    "for params in ParameterGrid(param_grid):\n",
    "    accuracy, report = train_and_evaluate(params)\n",
    "    if accuracy > best_accuracy:\n",
    "        best_accuracy = accuracy\n",
    "        best_params = params\n",
    "        best_report = report\n",
    "\n",
    "print(\"Best parameters found: \", best_params)\n",
    "print(f\"Best Model Accuracy: {best_accuracy}\")\n",
    "print(\"Classification Report:\")\n",
    "print(best_report)"
   ],
   "id": "df896d79e0be83c0",
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'int' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[6], line 4\u001B[0m\n\u001B[1;32m      1\u001B[0m X_train, X_valid, y_train, y_valid \u001B[38;5;241m=\u001B[39m data_train, data_valid, labels_train, labels_valid\n\u001B[1;32m      3\u001B[0m \u001B[38;5;66;03m# Create DataLoader\u001B[39;00m\n\u001B[0;32m----> 4\u001B[0m train_dataset \u001B[38;5;241m=\u001B[39m TensorDataset(X_train, y_train)\n\u001B[1;32m      5\u001B[0m train_loader \u001B[38;5;241m=\u001B[39m DataLoader(train_dataset, batch_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m32\u001B[39m, shuffle\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[1;32m      7\u001B[0m \u001B[38;5;66;03m# Define a simple model\u001B[39;00m\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.11/site-packages/torch/utils/data/dataset.py:204\u001B[0m, in \u001B[0;36mTensorDataset.__init__\u001B[0;34m(self, *tensors)\u001B[0m\n\u001B[1;32m    203\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39mtensors: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m--> 204\u001B[0m     \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mall\u001B[39m(tensors[\u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m.\u001B[39msize(\u001B[38;5;241m0\u001B[39m) \u001B[38;5;241m==\u001B[39m tensor\u001B[38;5;241m.\u001B[39msize(\u001B[38;5;241m0\u001B[39m) \u001B[38;5;28;01mfor\u001B[39;00m tensor \u001B[38;5;129;01min\u001B[39;00m tensors), \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mSize mismatch between tensors\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    205\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtensors \u001B[38;5;241m=\u001B[39m tensors\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.11/site-packages/torch/utils/data/dataset.py:204\u001B[0m, in \u001B[0;36m<genexpr>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m    203\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39mtensors: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m--> 204\u001B[0m     \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mall\u001B[39m(tensors[\u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m.\u001B[39msize(\u001B[38;5;241m0\u001B[39m) \u001B[38;5;241m==\u001B[39m tensor\u001B[38;5;241m.\u001B[39msize(\u001B[38;5;241m0\u001B[39m) \u001B[38;5;28;01mfor\u001B[39;00m tensor \u001B[38;5;129;01min\u001B[39;00m tensors), \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mSize mismatch between tensors\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    205\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtensors \u001B[38;5;241m=\u001B[39m tensors\n",
      "\u001B[0;31mTypeError\u001B[0m: 'int' object is not callable"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-05-15T14:48:45.736635Z",
     "start_time": "2024-05-15T14:46:54.893625Z"
    }
   },
   "source": [
    "X_train = data_train.reshape(data_train.shape[0], -1)\n",
    "X_valid = data_valid.reshape(data_valid.shape[0], -1)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_valid = scaler.transform(X_valid)\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define parameter grid\n",
    "param_grid = {\n",
    "    'alpha': [0.0001, 0.001, 0.01, 0.1],\n",
    "    'max_iter': [1000, 2000, 3000],\n",
    "    'learning_rate': ['constant', 'optimal', 'invscaling', 'adaptive'],\n",
    "    'eta0': [0.001, 0.01, 0.1],\n",
    "    'penalty': ['l2', 'l1', 'elasticnet']\n",
    "}\n",
    "\n",
    "# Initialize SGDClassifier\n",
    "sgd_model = SGDClassifier(loss='hinge')\n",
    "\n",
    "# Perform grid search\n",
    "grid_search = GridSearchCV(sgd_model, param_grid, cv=5, n_jobs=-1, verbose=3)\n",
    "grid_search.fit(X_train, labels_train)\n",
    "\n",
    "# Best parameters and estimator\n",
    "print(\"Best parameters found: \", grid_search.best_params_)\n",
    "best_sgd_model = grid_search.best_estimator_\n",
    "\n",
    "# Evaluate the best model\n",
    "y_pred = best_sgd_model.predict(X_valid)\n",
    "accuracy = accuracy_score(labels_valid, y_pred)\n",
    "report = classification_report(labels_valid, y_pred)\n",
    "\n",
    "print(f\"Best Model Accuracy: {accuracy}\")\n",
    "print(\"Classification Report:\")\n",
    "print(report)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 432 candidates, totalling 2160 fits\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[8], line 25\u001B[0m\n\u001B[1;32m     23\u001B[0m \u001B[38;5;66;03m# Perform grid search\u001B[39;00m\n\u001B[1;32m     24\u001B[0m grid_search \u001B[38;5;241m=\u001B[39m GridSearchCV(sgd_model, param_grid, cv\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m5\u001B[39m, n_jobs\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m, verbose\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m3\u001B[39m)\n\u001B[0;32m---> 25\u001B[0m grid_search\u001B[38;5;241m.\u001B[39mfit(X_train, labels_train)\n\u001B[1;32m     27\u001B[0m \u001B[38;5;66;03m# Best parameters and estimator\u001B[39;00m\n\u001B[1;32m     28\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mBest parameters found: \u001B[39m\u001B[38;5;124m\"\u001B[39m, grid_search\u001B[38;5;241m.\u001B[39mbest_params_)\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/model_selection/_search.py:874\u001B[0m, in \u001B[0;36mBaseSearchCV.fit\u001B[0;34m(self, X, y, groups, **fit_params)\u001B[0m\n\u001B[1;32m    868\u001B[0m     results \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_format_results(\n\u001B[1;32m    869\u001B[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001B[1;32m    870\u001B[0m     )\n\u001B[1;32m    872\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m results\n\u001B[0;32m--> 874\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_run_search(evaluate_candidates)\n\u001B[1;32m    876\u001B[0m \u001B[38;5;66;03m# multimetric is determined here because in the case of a callable\u001B[39;00m\n\u001B[1;32m    877\u001B[0m \u001B[38;5;66;03m# self.scoring the return type is only known after calling\u001B[39;00m\n\u001B[1;32m    878\u001B[0m first_test_score \u001B[38;5;241m=\u001B[39m all_out[\u001B[38;5;241m0\u001B[39m][\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtest_scores\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/model_selection/_search.py:1388\u001B[0m, in \u001B[0;36mGridSearchCV._run_search\u001B[0;34m(self, evaluate_candidates)\u001B[0m\n\u001B[1;32m   1386\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_run_search\u001B[39m(\u001B[38;5;28mself\u001B[39m, evaluate_candidates):\n\u001B[1;32m   1387\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Search all candidates in param_grid\"\"\"\u001B[39;00m\n\u001B[0;32m-> 1388\u001B[0m     evaluate_candidates(ParameterGrid(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mparam_grid))\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/model_selection/_search.py:821\u001B[0m, in \u001B[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001B[0;34m(candidate_params, cv, more_results)\u001B[0m\n\u001B[1;32m    813\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mverbose \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[1;32m    814\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\n\u001B[1;32m    815\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFitting \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;124m folds for each of \u001B[39m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;124m candidates,\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    816\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m totalling \u001B[39m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m fits\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(\n\u001B[1;32m    817\u001B[0m             n_splits, n_candidates, n_candidates \u001B[38;5;241m*\u001B[39m n_splits\n\u001B[1;32m    818\u001B[0m         )\n\u001B[1;32m    819\u001B[0m     )\n\u001B[0;32m--> 821\u001B[0m out \u001B[38;5;241m=\u001B[39m parallel(\n\u001B[1;32m    822\u001B[0m     delayed(_fit_and_score)(\n\u001B[1;32m    823\u001B[0m         clone(base_estimator),\n\u001B[1;32m    824\u001B[0m         X,\n\u001B[1;32m    825\u001B[0m         y,\n\u001B[1;32m    826\u001B[0m         train\u001B[38;5;241m=\u001B[39mtrain,\n\u001B[1;32m    827\u001B[0m         test\u001B[38;5;241m=\u001B[39mtest,\n\u001B[1;32m    828\u001B[0m         parameters\u001B[38;5;241m=\u001B[39mparameters,\n\u001B[1;32m    829\u001B[0m         split_progress\u001B[38;5;241m=\u001B[39m(split_idx, n_splits),\n\u001B[1;32m    830\u001B[0m         candidate_progress\u001B[38;5;241m=\u001B[39m(cand_idx, n_candidates),\n\u001B[1;32m    831\u001B[0m         \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mfit_and_score_kwargs,\n\u001B[1;32m    832\u001B[0m     )\n\u001B[1;32m    833\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m (cand_idx, parameters), (split_idx, (train, test)) \u001B[38;5;129;01min\u001B[39;00m product(\n\u001B[1;32m    834\u001B[0m         \u001B[38;5;28menumerate\u001B[39m(candidate_params), \u001B[38;5;28menumerate\u001B[39m(cv\u001B[38;5;241m.\u001B[39msplit(X, y, groups))\n\u001B[1;32m    835\u001B[0m     )\n\u001B[1;32m    836\u001B[0m )\n\u001B[1;32m    838\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(out) \u001B[38;5;241m<\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[1;32m    839\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m    840\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNo fits were performed. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    841\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mWas the CV iterator empty? \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    842\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mWere there no candidates?\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    843\u001B[0m     )\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/utils/parallel.py:63\u001B[0m, in \u001B[0;36mParallel.__call__\u001B[0;34m(self, iterable)\u001B[0m\n\u001B[1;32m     58\u001B[0m config \u001B[38;5;241m=\u001B[39m get_config()\n\u001B[1;32m     59\u001B[0m iterable_with_config \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m     60\u001B[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001B[1;32m     61\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m delayed_func, args, kwargs \u001B[38;5;129;01min\u001B[39;00m iterable\n\u001B[1;32m     62\u001B[0m )\n\u001B[0;32m---> 63\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__call__\u001B[39m(iterable_with_config)\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.11/site-packages/joblib/parallel.py:1098\u001B[0m, in \u001B[0;36mParallel.__call__\u001B[0;34m(self, iterable)\u001B[0m\n\u001B[1;32m   1095\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_iterating \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[1;32m   1097\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backend\u001B[38;5;241m.\u001B[39mretrieval_context():\n\u001B[0;32m-> 1098\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mretrieve()\n\u001B[1;32m   1099\u001B[0m \u001B[38;5;66;03m# Make sure that we get a last message telling us we are done\u001B[39;00m\n\u001B[1;32m   1100\u001B[0m elapsed_time \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mtime() \u001B[38;5;241m-\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_start_time\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.11/site-packages/joblib/parallel.py:975\u001B[0m, in \u001B[0;36mParallel.retrieve\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    973\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    974\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mgetattr\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backend, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124msupports_timeout\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;28;01mFalse\u001B[39;00m):\n\u001B[0;32m--> 975\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_output\u001B[38;5;241m.\u001B[39mextend(job\u001B[38;5;241m.\u001B[39mget(timeout\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtimeout))\n\u001B[1;32m    976\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    977\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_output\u001B[38;5;241m.\u001B[39mextend(job\u001B[38;5;241m.\u001B[39mget())\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.11/site-packages/joblib/_parallel_backends.py:567\u001B[0m, in \u001B[0;36mLokyBackend.wrap_future_result\u001B[0;34m(future, timeout)\u001B[0m\n\u001B[1;32m    564\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Wrapper for Future.result to implement the same behaviour as\u001B[39;00m\n\u001B[1;32m    565\u001B[0m \u001B[38;5;124;03mAsyncResults.get from multiprocessing.\"\"\"\u001B[39;00m\n\u001B[1;32m    566\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 567\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m future\u001B[38;5;241m.\u001B[39mresult(timeout\u001B[38;5;241m=\u001B[39mtimeout)\n\u001B[1;32m    568\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m CfTimeoutError \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    569\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTimeoutError\u001B[39;00m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01me\u001B[39;00m\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.11/concurrent/futures/_base.py:451\u001B[0m, in \u001B[0;36mFuture.result\u001B[0;34m(self, timeout)\u001B[0m\n\u001B[1;32m    448\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_state \u001B[38;5;241m==\u001B[39m FINISHED:\n\u001B[1;32m    449\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m__get_result()\n\u001B[0;32m--> 451\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_condition\u001B[38;5;241m.\u001B[39mwait(timeout)\n\u001B[1;32m    453\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_state \u001B[38;5;129;01min\u001B[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n\u001B[1;32m    454\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m CancelledError()\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.11/threading.py:327\u001B[0m, in \u001B[0;36mCondition.wait\u001B[0;34m(self, timeout)\u001B[0m\n\u001B[1;32m    325\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:    \u001B[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001B[39;00m\n\u001B[1;32m    326\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m timeout \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m--> 327\u001B[0m         waiter\u001B[38;5;241m.\u001B[39macquire()\n\u001B[1;32m    328\u001B[0m         gotit \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m    329\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-15T14:29:42.032589Z",
     "start_time": "2024-05-15T14:29:41.754734Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "# Example data\n",
    "\n",
    "# Split the data\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create DataLoader\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Define a simple model\n",
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(SimpleModel, self).__init__()\n",
    "        self.linear = nn.Linear(input_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "# Set device to MPS if available\n",
    "device = torch.device(\"mps\" if torch.has_mps else \"cpu\")\n",
    "\n",
    "# Define the function to train and evaluate the model\n",
    "def train_and_evaluate(params):\n",
    "    model = SimpleModel(input_dim=X_train.shape[1]).to(device)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=params['lr'], weight_decay=params['weight_decay'])\n",
    "\n",
    "    # Training loop\n",
    "    model.train()\n",
    "    for epoch in range(params['epochs']):\n",
    "        for batch_X, batch_y in train_loader:\n",
    "            batch_X, batch_y = batch_X.to(device), batch_y.to(device).float()\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_X).squeeze()\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        X_valid_device = X_valid.to(device)\n",
    "        y_valid_device = y_valid.to(device).float()\n",
    "        outputs = model(X_valid_device).squeeze()\n",
    "        preds = (torch.sigmoid(outputs) > 0.5).int()\n",
    "        accuracy = accuracy_score(y_valid_device.cpu(), preds.cpu())\n",
    "        report = classification_report(y_valid_device.cpu(), preds.cpu())\n",
    "    \n",
    "    return accuracy, report\n",
    "\n",
    "# Define parameter grid\n",
    "param_grid = {\n",
    "    'lr': [0.001, 0.01, 0.1],\n",
    "    'weight_decay': [0.0001, 0.001, 0.01],\n",
    "    'epochs': [10, 20, 30]\n",
    "}\n",
    "\n",
    "# Perform grid search manually\n",
    "best_params = None\n",
    "best_accuracy = 0\n",
    "best_report = None\n",
    "\n",
    "for params in ParameterGrid(param_grid):\n",
    "    accuracy, report = train_and_evaluate(params)\n",
    "    if accuracy > best_accuracy:\n",
    "        best_accuracy = accuracy\n",
    "        best_params = params\n",
    "        best_report = report\n",
    "\n",
    "print(\"Best parameters found: \", best_params)\n",
    "print(f\"Best Model Accuracy: {best_accuracy}\")\n",
    "print(\"Classification Report:\")\n",
    "print(best_report)\n"
   ],
   "id": "40048b8f6e19bb08",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[5], line 4\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# Example data\u001B[39;00m\n\u001B[1;32m      2\u001B[0m \n\u001B[1;32m      3\u001B[0m \u001B[38;5;66;03m# Split the data\u001B[39;00m\n\u001B[0;32m----> 4\u001B[0m X_train, X_valid, y_train, y_valid \u001B[38;5;241m=\u001B[39m train_test_split(X, y, test_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.2\u001B[39m, random_state\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m42\u001B[39m)\n\u001B[1;32m      6\u001B[0m \u001B[38;5;66;03m# Create DataLoader\u001B[39;00m\n\u001B[1;32m      7\u001B[0m train_dataset \u001B[38;5;241m=\u001B[39m TensorDataset(X_train, y_train)\n",
      "\u001B[0;31mNameError\u001B[0m: name 'X' is not defined"
     ]
    }
   ],
   "execution_count": 5
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
